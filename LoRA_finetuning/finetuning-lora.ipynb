{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":98084,"databundleVersionId":11711500,"sourceType":"competition"}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T02:26:42.626505Z","iopub.execute_input":"2025-04-19T02:26:42.627194Z","iopub.status.idle":"2025-04-19T02:26:42.636473Z","shell.execute_reply.started":"2025-04-19T02:26:42.627171Z","shell.execute_reply":"2025-04-19T02:26:42.635816Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install Hugging Face Transformers and other necessary libraries\n!pip install -q transformers datasets peft accelerate bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T00:01:03.320985Z","iopub.execute_input":"2025-04-22T00:01:03.321180Z","iopub.status.idle":"2025-04-22T00:02:42.943929Z","shell.execute_reply.started":"2025-04-22T00:01:03.321162Z","shell.execute_reply":"2025-04-22T00:02:42.943095Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Importing required libraries for data processing, model building, and evaluation\nimport os, math, torch\nimport numpy as np\nimport pandas as pd\nimport pickle\n\nfrom datasets import concatenate_datasets, load_dataset, Dataset\nfrom transformers import (\n    AutoTokenizer, AutoModelForSequenceClassification,\n    TrainingArguments, Trainer, TrainerCallback,\n    DataCollatorWithPadding\n)\nfrom torch.nn import functional as F\nfrom sklearn.metrics import accuracy_score\nfrom peft import get_peft_model, AdaLoraConfig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T02:26:43.332291Z","iopub.execute_input":"2025-04-19T02:26:43.332788Z","iopub.status.idle":"2025-04-19T02:26:50.766615Z","shell.execute_reply.started":"2025-04-19T02:26:43.332768Z","shell.execute_reply":"2025-04-19T02:26:50.765918Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nBASE_MODEL    = \"roberta-base\"\nTEACHER_MODEL = None  # no distillation here","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load & tokenize AG News\ntok = AutoTokenizer.from_pretrained(BASE_MODEL)\nraw = load_dataset(\"ag_news\")\ndef prep(batch):\n    return tok(batch[\"text\"], truncation=True, padding=False, max_length=256)\n\ntoked = raw.map(prep, batched=True, remove_columns=[\"text\"])\ntoked = toked.rename_column(\"label\", \"labels\")\ntoked[\"train\"] = toked[\"train\"].map(lambda x: {\"labels\": int(x[\"labels\"])})\ntoked[\"test\"]  = toked[\"test\"].map(lambda x: {\"labels\": int(x[\"labels\"])})\n\nsplit = toked[\"train\"].train_test_split(test_size=640, seed=42, stratify_by_column=\"labels\")\ntrain_ds, val_ds = split[\"train\"], split[\"test\"]\ncollator = DataCollatorWithPadding(tok)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Build model with AdaLoRA\nstudent = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# compute total steps\nNUM_EPOCHS = 3\nBATCH_SIZE = 32\nsteps_per_epoch = math.ceil(len(train_ds) / BATCH_SIZE)\nTOTAL_STEPS = NUM_EPOCHS * steps_per_epoch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T02:26:50.767826Z","iopub.execute_input":"2025-04-19T02:26:50.768085Z","iopub.status.idle":"2025-04-19T02:26:50.815380Z","shell.execute_reply.started":"2025-04-19T02:26:50.768065Z","shell.execute_reply":"2025-04-19T02:26:50.814611Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ada_cfg = AdaLoraConfig(\n    init_r=11,\n    target_r=8,\n    lora_alpha=16,\n    lora_dropout=0.15,\n    target_modules=[\"query\",\"value\"],\n    bias=\"none\",\n    modules_to_save=[\"classifier\"],\n    total_step=TOTAL_STEPS,\n    tinit=0,\n    tfinal=TOTAL_STEPS,\n    deltaT=TOTAL_STEPS+1,\n    beta1=0.9,\n    beta2=0.999,\n    task_type=\"SEQ_CLS\"\n)\npeft_model = get_peft_model(student, ada_cfg)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print trainable params\ntotal = sum(p.numel() for p in peft_model.parameters())\ntrainable = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\nprint(f\"ğŸ”¢ Total params: {total:,} â€” Trainable: {trainable:,}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Callback to Excelâ€‘log perâ€‘epoch\nclass EpochLogger(TrainerCallback):\n    def __init__(self, path=\"results/epoch_metrics.xlsx\"):\n        self.path    = path\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n        # epoch â†’ {\"train_loss\": float or None, \"eval_accuracy\": float or None}\n        self.metrics = {}\n\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        logs = logs or {}\n        # Use integer epoch\n        epoch = int(state.epoch or 0)\n\n        # Initialize if needed\n        if epoch not in self.metrics:\n            self.metrics[epoch] = {\"train_loss\": None, \"eval_accuracy\": None}\n\n        # Update whichever values are present\n        if \"loss\" in logs:\n            self.metrics[epoch][\"train_loss\"] = logs[\"loss\"]\n        if \"eval_accuracy\" in logs:\n            self.metrics[epoch][\"eval_accuracy\"] = logs[\"eval_accuracy\"]\n\n        # Write out whole table to Excel\n        df = pd.DataFrame([\n            {\"epoch\": e, **vals}\n            for e, vals in sorted(self.metrics.items())\n        ])\n        df.to_excel(self.path, index=False)\n\n        # If both metrics are now available for this epoch, print once\n        row = self.metrics[epoch]\n        if row[\"train_loss\"] is not None and row[\"eval_accuracy\"] is not None:\n            print(\n                f\"[epoch {epoch}] \"\n                f\"loss={row['train_loss']:.4f}  \"\n                f\"acc={row['eval_accuracy']:.4f}\"\n            )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T02:26:54.541407Z","iopub.execute_input":"2025-04-19T02:26:54.542063Z","iopub.status.idle":"2025-04-19T03:30:12.086887Z","shell.execute_reply.started":"2025-04-19T02:26:54.542038Z","shell.execute_reply":"2025-04-19T03:30:12.086139Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TrainingArguments\ntraining_args = TrainingArguments(\n    output_dir=\"results\",\n    num_train_epochs=NUM_EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=64,\n    learning_rate=2e-4,\n    warmup_ratio=0.1,\n    weight_decay=0.01,\n    fp16=True,\n\n    do_train=True,\n    do_eval=True,\n    eval_strategy=\"epoch\",   # run eval at end of each epoch\n    logging_strategy=\"epoch\",# log loss at end of each epoch\n    save_strategy=\"epoch\",   # save checkpoint each epoch\n    save_total_limit=2,\n\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_accuracy\",\n    greater_is_better=True,\n\n    logging_steps=50,\n    report_to=\"none\",\n    label_names=[\"labels\"]\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Metrics fn\ndef compute_metrics(p):\n    preds = p.predictions.argmax(-1)\n    return {\"eval_accuracy\": accuracy_score(p.label_ids, preds)}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize Trainer & train\ntrainer = Trainer(\n    model=peft_model,\n    args=training_args,\n    train_dataset=train_ds,\n    eval_dataset=val_ds,\n    tokenizer=tok,\n    data_collator=collator,\n    compute_metrics=compute_metrics,\n    callbacks=[EpochLogger()]\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()\ntrainer.save_model(\"best_model\")\ntok.save_pretrained(\"best_model\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference on unlabelled using best_model","metadata":{}},{"cell_type":"code","source":"# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load tokenizer & collator from best_model/\ntokenizer = AutoTokenizer.from_pretrained(\"best_model\")\ncollator  = DataCollatorWithPadding(tokenizer, return_tensors=\"pt\")\n\n# Load unlabelled test\nwith open(\"/kaggle/input/deep-learning-spring-2025-project-2/test_unlabelled.pkl\",\"rb\") as f:\n    test_data = pickle.load(f)\ntest_ds = Dataset.from_dict({\"text\": test_data[\"text\"]})\n\n# Tokenize\ndef tokenize_fn(batch):\n    return tokenizer(batch[\"text\"], truncation=True, padding=True, max_length=256)\n\ntest_tok = test_ds.map(\n    tokenize_fn,\n    batched=True,\n    remove_columns=[\"text\"]\n)\n\n# Create DataLoader\ntest_loader = DataLoader(\n    test_tok,\n    batch_size=64,\n    shuffle=False,\n    collate_fn=collator\n)\n\n# Load AdaLoRA model from best_model/\n# This merges the adapters into the base model.\nbase_model = AutoModelForSequenceClassification.from_pretrained(\n    \"roberta-base\", num_labels=4\n)\nmodel = PeftModel.from_pretrained(base_model, \"best_model\") \\\n                 .merge_and_unload() \\\n                 .to(device)\nmodel.eval()\n\n# Run inference\nall_ids, all_preds = [], []\nbs = test_loader.batch_size\n\nfor i, batch in enumerate(test_loader):\n    batch = {k: v.to(device) for k, v in batch.items()}\n    with torch.no_grad():\n        logits = model(**batch).logits\n    preds = logits.argmax(dim=-1).cpu().tolist()\n\n    start = i * bs\n    all_ids.extend(range(start, start + len(preds)))\n    all_preds.extend(preds)\n\n# Save submission\nsubmission = pd.DataFrame({\"ID\": all_ids, \"label\": all_preds})\nsubmission.to_csv(\"best_model-submission.csv\", index=False)\nprint(\"Saved submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T03:34:18.079140Z","iopub.execute_input":"2025-04-19T03:34:18.079883Z","iopub.status.idle":"2025-04-19T03:36:17.660714Z","shell.execute_reply.started":"2025-04-19T03:34:18.079857Z","shell.execute_reply":"2025-04-19T03:36:17.660107Z"}},"outputs":[],"execution_count":null}]}