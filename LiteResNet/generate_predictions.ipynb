{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-13T15:05:41.973327Z",
     "iopub.status.busy": "2025-03-13T15:05:41.973056Z",
     "iopub.status.idle": "2025-03-13T15:05:41.995792Z",
     "shell.execute_reply": "2025-03-13T15:05:41.994982Z",
     "shell.execute_reply.started": "2025-03-13T15:05:41.973299Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/finetuned-with-mixup-and-cutmix/pytorch/default/1/best_model_finetuned (3).pth\n",
      "/kaggle/input/finetuned-model-91/pytorch/default/1/best_model_finetuned (2).pth\n",
      "/kaggle/input/without-finetuning/pytorch/default/1/best_model (12).pth\n",
      "/kaggle/input/latest_finetune/pytorch/default/1/best_model_finetuned.pth\n",
      "/kaggle/input/retesting_previous_with_contrastedge-in-tta/pytorch/default/1/best_model_finetuned (4).pth\n",
      "/kaggle/input/finetuned_98/pytorch/default/1/best_model_finetuned (1).pth\n",
      "/kaggle/input/best_model_without_finetune/pytorch/default/1/best_model (13).pth\n",
      "/kaggle/input/deep-learning-spring-2025-project-1/cifar_test_nolabel.pkl\n",
      "/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py/data_batch_1\n",
      "/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py/data_batch_2\n",
      "/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py/batches.meta\n",
      "/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py/test_batch\n",
      "/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py/data_batch_3\n",
      "/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py/data_batch_5\n",
      "/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py/data_batch_4\n",
      "/kaggle/input/deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py/readme.html\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T15:05:45.192515Z",
     "iopub.status.busy": "2025-03-13T15:05:45.192149Z",
     "iopub.status.idle": "2025-03-13T15:05:48.382239Z",
     "shell.execute_reply": "2025-03-13T15:05:48.381251Z",
     "shell.execute_reply.started": "2025-03-13T15:05:45.192482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "!pip install torchsummary\n",
    "import torchsummary\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "import torch.nn.init as init\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.nn.utils import (\n",
    "  parameters_to_vector as Params2Vec,\n",
    "  vector_to_parameters as Vec2Params\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms.v2 as v2\n",
    "import random\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.optim.lr_scheduler import LambdaLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T15:05:48.383678Z",
     "iopub.status.busy": "2025-03-13T15:05:48.383345Z",
     "iopub.status.idle": "2025-03-13T15:05:48.731604Z",
     "shell.execute_reply": "2025-03-13T15:05:48.730658Z",
     "shell.execute_reply.started": "2025-03-13T15:05:48.383646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded test images shape: (10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "test_data_path = \"/kaggle/input/deep-learning-spring-2025-project-1/cifar_test_nolabel.pkl\"\n",
    "with open(test_data_path, 'rb') as file:\n",
    "    test_data = pickle.load(file, encoding='bytes')\n",
    "\n",
    "# Extract test images and IDs\n",
    "test_images = test_data[b'data']  # Shape (10000, 32, 32, 3) in numpy format\n",
    "test_ids = test_data[b'ids']        # Test image IDs\n",
    "\n",
    "print(f\"Loaded test images shape: {test_images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T15:05:48.733046Z",
     "iopub.status.busy": "2025-03-13T15:05:48.732706Z",
     "iopub.status.idle": "2025-03-13T15:05:48.741365Z",
     "shell.execute_reply": "2025-03-13T15:05:48.740526Z",
     "shell.execute_reply.started": "2025-03-13T15:05:48.733017Z"
    }
   },
   "outputs": [],
   "source": [
    "def detect_weak_edges(image, threshold=50):\n",
    "    \"\"\"\n",
    "    Detect weak edges using Sobel filters in PyTorch.\n",
    "    \n",
    "    Args:\n",
    "        image (torch.Tensor): Input image tensor (C, H, W).\n",
    "        threshold (int): Edge strength threshold to determine weak edges.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if edges are weak, False otherwise.\n",
    "    \"\"\"\n",
    "    # Convert RGB to grayscale using ITU-R BT.601 standard\n",
    "    grayscale = 0.299 * image[0] + 0.587 * image[1] + 0.114 * image[2]  # Shape: (H, W)\n",
    "\n",
    "    # Apply Sobel filters for edge detection\n",
    "    sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(image.device)\n",
    "    sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(image.device)\n",
    "\n",
    "    # Reshape grayscale to (1, 1, H, W) for convolution\n",
    "    grayscale = grayscale.unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, H, W)\n",
    "\n",
    "    # Apply Sobel convolution\n",
    "    edge_x = F.conv2d(grayscale, sobel_x, padding=1)\n",
    "    edge_y = F.conv2d(grayscale, sobel_y, padding=1)\n",
    "    \n",
    "    # Compute edge strength\n",
    "    edges = torch.sqrt(edge_x ** 2 + edge_y ** 2)  # Edge magnitude\n",
    "    edge_sum = edges.sum().item()  # Sum of edge intensities\n",
    "\n",
    "    return edge_sum < threshold  # Return True if edges are weak\n",
    "\n",
    "def enhance_contrast(image):\n",
    "    \"\"\"\n",
    "    Enhance image contrast using PyTorch's histogram equalization.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): Input image tensor (C, H, W).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Contrast-enhanced image tensor (C, H, W).\n",
    "    \"\"\"\n",
    "    # Convert to grayscale for histogram equalization\n",
    "    grayscale = 0.299 * image[0] + 0.587 * image[1] + 0.114 * image[2]  # Shape: (H, W)\n",
    "\n",
    "    # Compute histogram and cumulative distribution function (CDF)\n",
    "    hist = torch.histc(grayscale, bins=256, min=0.0, max=1.0)\n",
    "    cdf = hist.cumsum(0) / hist.sum()  # Normalize CDF\n",
    "\n",
    "    # Apply histogram equalization\n",
    "    equalized = torch.index_select(cdf, 0, (grayscale * 255).long().clamp(0, 255))  # Apply equalization\n",
    "    equalized = equalized / equalized.max()  # Normalize\n",
    "\n",
    "    # Scale RGB channels based on equalized grayscale\n",
    "    enhanced_image = image * (equalized / (grayscale + 1e-6))  # Avoid division by zero\n",
    "    enhanced_image = torch.clamp(enhanced_image, 0, 1)  # Ensure valid range\n",
    "\n",
    "    return enhanced_image\n",
    "\n",
    "class ContrastEnhancementTransform(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Custom PyTorch transform to selectively enhance contrast for weak edge images.\n",
    "    \"\"\"\n",
    "    def forward(self, img):\n",
    "        \"\"\"\n",
    "        Apply contrast enhancement selectively based on weak edge detection.\n",
    "\n",
    "        Args:\n",
    "            img (torch.Tensor): Input image tensor (C, H, W).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Transformed image tensor (C, H, W).\n",
    "        \"\"\"\n",
    "        if detect_weak_edges(img):  # Apply only to weak edge images\n",
    "            img = enhance_contrast(img)\n",
    "\n",
    "        return img  # No conversion needed (still a PyTorch tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T05:17:51.071016Z",
     "iopub.status.busy": "2025-03-13T05:17:51.070797Z",
     "iopub.status.idle": "2025-03-13T05:17:51.088184Z",
     "shell.execute_reply": "2025-03-13T05:17:51.087570Z",
     "shell.execute_reply.started": "2025-03-13T05:17:51.070996Z"
    }
   },
   "outputs": [],
   "source": [
    "#Test time augmentation\n",
    "tta_transforms = [\n",
    "    # 1. Original image: Baseline without any augmentation.\n",
    "    transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        ContrastEnhancementTransform(),  # Selectively enhance weak-edge images\n",
    "        transforms.Normalize(mean=[0.49139968, 0.48215827, 0.44653124],\n",
    "                             std=[0.24703233, 0.24348505, 0.26158768])\n",
    "    ]),\n",
    "\n",
    "    # 2. Horizontal Flip: Captures potential left-right symmetry.\n",
    "    transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        ContrastEnhancementTransform(),  # Selectively enhance weak-edge images\n",
    "        transforms.RandomHorizontalFlip(p=1.0),  # Always flip\n",
    "        transforms.Normalize(mean=[0.49139968, 0.48215827, 0.44653124],\n",
    "                             std=[0.24703233, 0.24348505, 0.26158768])\n",
    "    ]),\n",
    "\n",
    "    # 3. Fixed +5° Rotation: Accounts for slight rotation variations.\n",
    "    transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        ContrastEnhancementTransform(),  # Selectively enhance weak-edge images\n",
    "        transforms.RandomRotation(degrees=(5, 5)),\n",
    "        transforms.Normalize(mean=[0.49139968, 0.48215827, 0.44653124],\n",
    "                             std=[0.24703233, 0.24348505, 0.26158768])\n",
    "    ]),\n",
    "\n",
    "    # 4. Fixed -5° Rotation: Another slight rotation variant.\n",
    "    transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        ContrastEnhancementTransform(),  # Selectively enhance weak-edge images\n",
    "        transforms.RandomRotation(degrees=(-5, -5)),\n",
    "        transforms.Normalize(mean=[0.49139968, 0.48215827, 0.44653124],\n",
    "                             std=[0.24703233, 0.24348505, 0.26158768])\n",
    "    ]),\n",
    "\n",
    "    # 5. Center Crop Variant: Remove background bias by cropping and resizing.\n",
    "    transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        ContrastEnhancementTransform(),  # Selectively enhance weak-edge images\n",
    "        transforms.CenterCrop(28),  # Crop a central region\n",
    "        transforms.Resize(32),      # Resize back to original size\n",
    "        transforms.Normalize(mean=[0.49139968, 0.48215827, 0.44653124],\n",
    "                             std=[0.24703233, 0.24348505, 0.26158768])\n",
    "    ]),\n",
    "\n",
    "    # 6. Mild Random Resized Crop: Subtle positional variation.\n",
    "    transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        ContrastEnhancementTransform(),  # Selectively enhance weak-edge images\n",
    "        transforms.RandomResizedCrop(32, scale=(0.9, 1.0)),\n",
    "        transforms.Normalize(mean=[0.49139968, 0.48215827, 0.44653124],\n",
    "                             std=[0.24703233, 0.24348505, 0.26158768])\n",
    "    ]),\n",
    "\n",
    "    # 7. Slight Brightness/Contrast Adjustment: Very mild color variation.\n",
    "    transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        ContrastEnhancementTransform(),  # Selectively enhance weak-edge images\n",
    "        transforms.ColorJitter(brightness=0.05, contrast=0.05),\n",
    "        transforms.Normalize(mean=[0.49139968, 0.48215827, 0.44653124],\n",
    "                             std=[0.24703233, 0.24348505, 0.26158768])\n",
    "    ])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T15:05:54.735025Z",
     "iopub.status.busy": "2025-03-13T15:05:54.734703Z",
     "iopub.status.idle": "2025-03-13T15:05:54.740612Z",
     "shell.execute_reply": "2025-03-13T15:05:54.739695Z",
     "shell.execute_reply.started": "2025-03-13T15:05:54.734999Z"
    }
   },
   "outputs": [],
   "source": [
    "# Custom Test Dataset for CIFAR-10\n",
    "class TestCIFAR10Dataset(Dataset):\n",
    "    def __init__(self, images, ids, transform=None):\n",
    "        self.images = images\n",
    "        self.ids = ids\n",
    "        self.transform = transform  # No transform applied during dataset init\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        # Convert numpy array to PIL image if necessary\n",
    "        if isinstance(img, np.ndarray):\n",
    "            # If image is channel-first, convert to channel-last\n",
    "            if img.shape[0] == 3:\n",
    "                img = np.transpose(img, (1, 2, 0))\n",
    "            # Ensure image is in uint8 format\n",
    "            if img.dtype != np.uint8:\n",
    "                img = img.astype(np.uint8)\n",
    "            img = Image.fromarray(img)\n",
    "        # Apply transform if provided (for TTA, transform is applied later)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, self.ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T15:05:57.164505Z",
     "iopub.status.busy": "2025-03-13T15:05:57.164198Z",
     "iopub.status.idle": "2025-03-13T15:05:57.168416Z",
     "shell.execute_reply": "2025-03-13T15:05:57.167490Z",
     "shell.execute_reply.started": "2025-03-13T15:05:57.164474Z"
    }
   },
   "outputs": [],
   "source": [
    "# Custom Collate Function\n",
    "def custom_collate_fn(batch):\n",
    "    images, ids = zip(*batch)\n",
    "    # Return as lists so each image remains as a PIL image\n",
    "    return list(images), list(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T15:05:57.405781Z",
     "iopub.status.busy": "2025-03-13T15:05:57.405544Z",
     "iopub.status.idle": "2025-03-13T15:05:57.410282Z",
     "shell.execute_reply": "2025-03-13T15:05:57.409350Z",
     "shell.execute_reply.started": "2025-03-13T15:05:57.405761Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create Dataset and DataLoader for Test Data\n",
    "batch_size = 64   # Keeping Same as training and fine tuning\n",
    "num_workers = 4   \n",
    "\n",
    "final_dataset = TestCIFAR10Dataset(test_images, test_ids, transform=None)\n",
    "final_loader = DataLoader(final_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=False,\n",
    "                          num_workers=num_workers,\n",
    "                          collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T15:05:57.598156Z",
     "iopub.status.busy": "2025-03-13T15:05:57.597859Z",
     "iopub.status.idle": "2025-03-13T15:05:57.606296Z",
     "shell.execute_reply": "2025-03-13T15:05:57.604762Z",
     "shell.execute_reply.started": "2025-03-13T15:05:57.598133Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_final_predictions_csv(model, best_model_path, device):\n",
    "    # Load best model weights and set model to evaluation mode\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for images, ids in final_loader:\n",
    "            batch_preds = []\n",
    "            \n",
    "            # Apply each TTA transform to the entire batch\n",
    "            for tta_transform in tta_transforms:\n",
    "                # Process each image individually with the current TTA transform\n",
    "                augmented_images = torch.stack([tta_transform(img) for img in images])\n",
    "                augmented_images = augmented_images.to(device)\n",
    "                outputs = model(augmented_images)\n",
    "                probs = F.softmax(outputs, dim=1)\n",
    "                batch_preds.append(probs.cpu())\n",
    "            \n",
    "            # Average the predictions from all TTA versions\n",
    "            avg_preds = torch.mean(torch.stack(batch_preds), dim=0)\n",
    "            _, final_preds = avg_preds.max(1)\n",
    "            \n",
    "            # Append the predictions for each image in the batch\n",
    "            for i in range(len(final_preds)):\n",
    "                predictions.append({\"ID\": ids[i], \"Labels\": final_preds[i].item()})\n",
    "\n",
    "    # Convert predictions to a DataFrame and save as CSV\n",
    "    df_predictions = pd.DataFrame(predictions)\n",
    "    output_csv = 'test_predictions_nearly_final_new_tta.csv'\n",
    "    df_predictions.to_csv(output_csv, index=False)\n",
    "    print(f\"Final predictions with TTA saved to: {output_csv}\")\n",
    "    return df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T15:05:59.637737Z",
     "iopub.status.busy": "2025-03-13T15:05:59.637452Z",
     "iopub.status.idle": "2025-03-13T15:05:59.650313Z",
     "shell.execute_reply": "2025-03-13T15:05:59.649495Z",
     "shell.execute_reply.started": "2025-03-13T15:05:59.637717Z"
    }
   },
   "outputs": [],
   "source": [
    "# Custom ResNet Model\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(channels, channels // reduction, bias=False)\n",
    "        self.fc2 = nn.Linear(channels // reduction, channels, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.adaptive_avg_pool2d(x, 1).view(x.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out).view(x.size(0), x.size(1), 1, 1)\n",
    "        return x * out\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64  \n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.seblock = SEBlock(channels=self.in_planes)\n",
    "        \n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1) \n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.avg_pool = nn.AvgPool2d(8)  \n",
    "        self.fc = nn.Linear(256 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.seblock(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T15:05:59.800368Z",
     "iopub.status.busy": "2025-03-13T15:05:59.800155Z",
     "iopub.status.idle": "2025-03-13T15:06:00.195839Z",
     "shell.execute_reply": "2025-03-13T15:06:00.195142Z",
     "shell.execute_reply.started": "2025-03-13T15:05:59.800351Z"
    }
   },
   "outputs": [],
   "source": [
    "def EfficientResNet():\n",
    "    return ResNet(BasicBlock, [7, 4, 3]) \n",
    "\n",
    "# Define Training and Evaluation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EfficientResNet().to(device)\n",
    "\n",
    "best_model_path = \"/kaggle/input/retesting_previous_with_contrastedge-in-tta/pytorch/default/1/best_model_finetuned.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T15:06:02.018314Z",
     "iopub.status.busy": "2025-03-13T15:06:02.018008Z",
     "iopub.status.idle": "2025-03-13T15:07:29.433242Z",
     "shell.execute_reply": "2025-03-13T15:07:29.432233Z",
     "shell.execute_reply.started": "2025-03-13T15:06:02.018288Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-bada4cfdacda>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final predictions with TTA saved to: test_predictions_nearly_final_new_tta.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>9995</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9996</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9997</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9998</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>9999</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID  Labels\n",
       "0        0       6\n",
       "1        1       1\n",
       "2        2       8\n",
       "3        3       6\n",
       "4        4       9\n",
       "...    ...     ...\n",
       "9995  9995       4\n",
       "9996  9996       0\n",
       "9997  9997       2\n",
       "9998  9998       5\n",
       "9999  9999       1\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate prediction csv\n",
    "final_predictions_df = generate_final_predictions_csv(model, best_model_path, device)\n",
    "final_predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11145869,
     "sourceId": 93057,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 263869,
     "modelInstanceId": 242252,
     "sourceId": 282720,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 264044,
     "modelInstanceId": 242424,
     "sourceId": 282914,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 264060,
     "modelInstanceId": 242437,
     "sourceId": 282933,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 264316,
     "modelInstanceId": 242697,
     "sourceId": 283247,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 264823,
     "modelInstanceId": 243216,
     "sourceId": 283841,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 264830,
     "modelInstanceId": 243223,
     "sourceId": 283848,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 265333,
     "modelInstanceId": 243711,
     "sourceId": 284391,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
